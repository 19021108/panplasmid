{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import fileinput\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_id(sample_id, contig_id):\n",
    "    '''\n",
    "    Generate a node_id from sample_id and contig_id.\n",
    "    '''\n",
    "    node_id = f\"{sample_id}:{contig_id}\"\n",
    "\n",
    "    return node_id\n",
    "\n",
    "def node_coverage(gfa_arguments, sequence_len):\n",
    "    \"\"\"\n",
    "    Return coverage parsed from dp or estimated from KC tag.\n",
    "    The second return value is True for dp and False for KC.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find dp or KC tag\n",
    "    for arguments in gfa_arguments:\n",
    "        # Check for the 'dp:f:' pattern or the 'KC:i:' pattern in each gfa_argument\n",
    "        dp_match = re.match(r'^dp:f:(.*)$', arguments)\n",
    "        kc_match = re.match(r'^KC:i:(.*)$', arguments)\n",
    "\n",
    "        if dp_match:\n",
    "            # If 'dp:f:' pattern is found, return the extracted coverage and True for dp\n",
    "            return float(dp_match.group(1)), True\n",
    "\n",
    "        if kc_match:\n",
    "            # If 'KC:i:' pattern is found, return the calculated coverage from KC and False for KC\n",
    "            return float(kc_match.group(1)) / sequence_len, False\n",
    "\n",
    "    # If neither 'dp' nor 'KC' tags are found, raise an assertion error\n",
    "    raise AssertionError(\"Depth not found\")\n",
    "\n",
    "def gc_content(sequence):\n",
    "    \"\"\"Calculate the GC content of a given DNA sequence.\"\"\"\n",
    "\n",
    "    # Count the number of 'G' and 'C' bases in the sequence\n",
    "    gc_bases = sum(base in 'GC' for base in sequence)\n",
    "\n",
    "    # Count the total number of valid bases (A, C, G, T) in the sequence\n",
    "    total_bases = sum(base in 'ACGT' for base in sequence)\n",
    "\n",
    "    # Calculate GC content only if there are valid bases in the sequence\n",
    "    if total_bases > 0:\n",
    "        gc_content = round(gc_bases / total_bases, 4)\n",
    "    else:\n",
    "        # Default GC content when there are no valid bases\n",
    "        gc_content = 0.5\n",
    "\n",
    "    return gc_content\n",
    "\n",
    "def kmer_distribution(sequence, kmer_len=5, scale=False):\n",
    "    \"\"\"Calculate k-mer distribution from a sequence\"\"\"\n",
    "\n",
    "    assert kmer_len % 2 == 1, \"K-mer length should be odd.\"\n",
    "\n",
    "    k_mers = [\"\".join(x) for x in itertools.product(\"ACGT\", repeat=kmer_len)]\n",
    "\n",
    "    forward_kmers = []\n",
    "    forward_kmer_set = set()\n",
    "    reverse_kmer_set = set()\n",
    "\n",
    "    for k_mer in k_mers:\n",
    "        if not ((k_mer in forward_kmer_set) or (k_mer in reverse_kmer_set)):\n",
    "            forward_kmers.append(k_mer)\n",
    "            forward_kmer_set.add(k_mer)\n",
    "            reverse_kmer_set.add(str(Seq(k_mer).reverse_complement()))\n",
    "\n",
    "    # Using defaultdict for pseudocounts\n",
    "    kmer_count_dict = defaultdict(lambda: 0.01)\n",
    "\n",
    "    # Counting k-mers in the sequence\n",
    "    for i in range(len(sequence) - kmer_len + 1):\n",
    "        kmer = sequence[i:i + kmer_len]\n",
    "\n",
    "        if kmer in kmer_count_dict:\n",
    "            kmer_count_dict[kmer] += 1\n",
    "\n",
    "    # Calculating k-mer distribution\n",
    "    k_mer_distribution = [\n",
    "        kmer_count_dict[k_mer] + kmer_count_dict[str(Seq(k_mer).reverse_complement())]\n",
    "        for k_mer in forward_kmers\n",
    "    ]\n",
    "\n",
    "    # Scaling k-mer distribution if specified\n",
    "    if scale:\n",
    "        total_count = sum(k_mer_distribution)\n",
    "        k_mer_distribution = [count / total_count for count in k_mer_distribution]\n",
    "\n",
    "    return k_mer_distribution\n",
    "\n",
    "def weighted_median(values, weights):\n",
    "    # Calculate the middle value\n",
    "    middle = np.sum(weights) / 2\n",
    "\n",
    "    # Calculate cumulative sum of weights\n",
    "    cumsum = np.cumsum(weights)\n",
    "\n",
    "    # Iterate through the cumulative sums\n",
    "    for i, x in enumerate(cumsum):\n",
    "        # Find the index where cumulative sum is greater than or equal to the middle\n",
    "        if x >= middle:\n",
    "            # Return the corresponding value as the weighted median\n",
    "            return values[i]\n",
    "\n",
    "    # Assertion to handle unexpected cases if the loop completes without returning\n",
    "    assert False\n",
    "\n",
    "def add_normalized_coverage(graph, node_ids):\n",
    "    \"\"\"\n",
    "    Add attribute coverage_norm: original coverage divided by median weighted by length.\n",
    "    \"\"\"\n",
    "    # Sort nodes based on their coverage attribute\n",
    "    sorted_nodes = sorted(node_ids, key=lambda x: graph.nodes[x][\"coverage\"])\n",
    "\n",
    "    # Extract lengths and coverages for sorted nodes\n",
    "    coverages = np.array([graph.nodes[node][\"coverage\"] for node in sorted_nodes])\n",
    "    lengths = np.array([graph.nodes[node][\"length\"] for node in sorted_nodes])\n",
    "\n",
    "    # Calculate the median\n",
    "    median = weighted_median(coverages, lengths)\n",
    "\n",
    "    # Calculate and add coverage_norm attribute for each node\n",
    "    for node_id in node_ids:\n",
    "        # Calculate coverage_norm: original coverage divided by the median weighted by length\n",
    "        graph.nodes[node_id][\"coverage_norm\"] = graph.nodes[node_id][\"coverage\"] / median\n",
    "\n",
    "def KL(a, b):\n",
    "    # Convert input arrays 'a' and 'b' to NumPy arrays of type float\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "\n",
    "    # Calculate the Kullback-Leibler divergence between probability distributions 'a' and 'b'\n",
    "    kl_divergence = np.sum(np.where(a != 0, a * np.log(a / b), 0))\n",
    "\n",
    "    return kl_divergence\n",
    "\n",
    "def label_to_pair(label):\n",
    "    \"\"\"Convert a label into a pair of values based on predefined mappings.\"\"\"\n",
    "\n",
    "    # Define mappings of labels to pairs\n",
    "    label_mappings = {\n",
    "        \"chromosome\": [0, 1],\n",
    "        \"plasmid\": [1, 0],\n",
    "        \"ambiguous\": [1, 1],\n",
    "        \"unlabeled\": [0, 0],\n",
    "        None: [0, 0]\n",
    "    }\n",
    "\n",
    "    if label in label_mappings:\n",
    "        return label_mappings[label]\n",
    "    else:\n",
    "        raise AssertionError(f\"Unrecognized label: {label}\")\n",
    "\n",
    "def pair_to_label(pair):\n",
    "    \"\"\"Converts a pair of values into a label based on predefined mappings\"\"\"\n",
    "\n",
    "    if pair == [0, 1]:\n",
    "        return \"chromosome\"\n",
    "    elif pair == [1, 0]:\n",
    "        return \"plasmid\"\n",
    "    elif pair == [1, 1]:\n",
    "        return \"ambiguous\"\n",
    "    elif pair == [0, 0]:\n",
    "        return \"unlabeled\"\n",
    "    else:\n",
    "        raise AssertionError(f\"Unrecognized pair: {pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph(graph_file, csv_file, sample_id, graph, minimum_contig_len):\n",
    "    node_ids = []\n",
    "    sequences = \"\"\n",
    "    coverage_types = {True:0, False:0}  # Which coverage types for individual nodes\n",
    "\n",
    "    with fileinput.input(graph_file, openhook=fileinput.hook_compressed, mode='r') as file:\n",
    "        for line in file:\n",
    "            if isinstance(line, bytes):\n",
    "                line = line.decode(\"utf-8\") # Convert byte to string\n",
    "\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "\n",
    "            if parts[0] == \"S\": # Node line\n",
    "                node_id = get_node_id(sample_id, parts[1])\n",
    "                node_ids.append(node_id)\n",
    "\n",
    "                sequence = parts[2].upper()\n",
    "\n",
    "                if not re.match(r'^[A-Z]*$', sequence):\n",
    "                    raise AssertionError(f\"Bad sequence in {node_id}\")\n",
    "\n",
    "                # N is ignored by GC, helps to avoid fake kmers\n",
    "                sequences += \"N\" + sequence\n",
    "\n",
    "                graph.add_node(node_id)\n",
    "                sequence_len = len(sequence)\n",
    "\n",
    "                # Calculate coverage\n",
    "                (coverage, is_dp) = node_coverage(parts[3:], sequence_len)\n",
    "\n",
    "                graph.nodes[node_id][\"contig\"] = parts[1]\n",
    "                graph.nodes[node_id][\"sample\"] = sample_id\n",
    "                graph.nodes[node_id][\"length\"] = sequence_len\n",
    "                graph.nodes[node_id][\"coverage\"] = coverage\n",
    "                graph.nodes[node_id][\"gc\"] = gc_content(sequence)\n",
    "                graph.nodes[node_id][\"kmer_counts_norm\"] = kmer_distribution(sequence, scale=True)\n",
    "\n",
    "                coverage_types[is_dp] += 1\n",
    "\n",
    "            if parts[0] == \"L\":  # Edge line\n",
    "                graph.add_edge(get_node_id(sample_id, parts[1]), get_node_id(sample_id, parts[3]))\n",
    "\n",
    "    # Check that only one coverage type seen\n",
    "    assert coverage_types[True] == 0 or coverage_types[False] == 0\n",
    "\n",
    "    # Get GC of whole sequence\n",
    "    gc_of_whole_seq = gc_content(sequences)\n",
    "\n",
    "    # Get max length\n",
    "    max_contig_length = max([graph.nodes[node_id][\"length\"] for node_id in node_ids])\n",
    "\n",
    "    # Get graph degrees and normalized gc content and normalized contig lengths (divided by max length)\n",
    "    for node_id in node_ids:\n",
    "        graph.nodes[node_id][\"degree\"] = graph.degree[node_id]\n",
    "        graph.nodes[node_id][\"gc_norm\"] =  graph.nodes[node_id][\"gc\"] - gc_of_whole_seq\n",
    "        graph.nodes[node_id][\"length_norm\"] = graph.nodes[node_id][\"length\"] / 2000000\n",
    "        graph.nodes[node_id][\"loglength\"] = math.log(graph.nodes[node_id][\"length\"] + 1)\n",
    "\n",
    "    add_normalized_coverage(graph, node_ids)\n",
    "\n",
    "    # Get euclidian of pentamer distribution for each node\n",
    "    all_kmer_counts_norm = np.array(kmer_distribution(sequences, scale=True))\n",
    "\n",
    "    for node_id in node_ids:\n",
    "        diff = np.array(graph.nodes[node_id][\"kmer_counts_norm\"]) - all_kmer_counts_norm\n",
    "        graph.nodes[node_id][\"kmer_dist\"] = np.linalg.norm(diff)\n",
    "        graph.nodes[node_id][\"kmer_dot\"] = np.dot(np.array(graph.nodes[node_id][\"kmer_counts_norm\"]), all_kmer_counts_norm)\n",
    "        graph.nodes[node_id][\"kmer_kl\"] = KL(np.array(graph.nodes[node_id][\"kmer_counts_norm\"]), all_kmer_counts_norm)\n",
    "\n",
    "    # Read and add node labels\n",
    "    if csv_file is not None:\n",
    "        labels_df = pd.read_csv(csv_file)\n",
    "        labels_df[\"id\"] = labels_df[\"contig\"].map(lambda x : get_node_id(sample_id, x))\n",
    "        labels_df.set_index(\"id\", inplace=True)\n",
    "    else:\n",
    "        labels_df = pd.DataFrame()\n",
    "\n",
    "    for node_id in node_ids:\n",
    "        label = None\n",
    "        if node_id in labels_df.index:\n",
    "            label = labels_df.loc[node_id, \"label\"]  # Textual label\n",
    "\n",
    "        pair = label_to_pair(label)  # Pair of binary values\n",
    "\n",
    "        graph.nodes[node_id][\"text_label\"] = pair_to_label(pair)\n",
    "        graph.nodes[node_id][\"plasmid_label\"] = pair[0]\n",
    "        graph.nodes[node_id][\"chrom_label\"] = pair[1]\n",
    "\n",
    "    # Get the number of nodes and edges\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "\n",
    "    print(\"Number of nodes:\", num_nodes)\n",
    "    print(\"Number of edges:\", num_edges)\n",
    "\n",
    "    # Remove short contigs from the graph and connect new neighbors\n",
    "    if minimum_contig_len > 0:\n",
    "        for node_id in node_ids:\n",
    "          if graph.nodes[node_id][\"length\"] < minimum_contig_len:\n",
    "              print(node_id)\n",
    "              neighbors = list(graph.neighbors(node_id))\n",
    "              all_new_edges = list(itertools.combinations(neighbors, 2))\n",
    "\n",
    "              for edge in all_new_edges:\n",
    "                  graph.add_edge(edge[0], edge[1])\n",
    "\n",
    "              graph.remove_node(node_id)\n",
    "\n",
    "    # Get the number of nodes and edges\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "\n",
    "    print(\"Number of nodes:\", num_nodes)\n",
    "    print(\"Number of edges:\", num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 223\n",
      "Number of edges: 265\n",
      "Number of nodes: 223\n",
      "Number of edges: 265\n"
     ]
    }
   ],
   "source": [
    "graph = nx.Graph()\n",
    "\n",
    "graph_file = '/workspaces/panplasmid/models/plasgraph2/example/SAMN15148288_SKESA.gfa.gz'\n",
    "csv_file = '/workspaces/panplasmid/models/plasgraph2/example/SAMN15148288_output.csv'\n",
    "sample_id = 'sample'\n",
    "\n",
    "read_graph(graph_file=graph_file, csv_file=csv_file, sample_id=sample_id, graph=graph, minimum_contig_len=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panplasmid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
